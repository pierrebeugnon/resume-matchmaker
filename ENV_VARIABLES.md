# üîë Variables d'Environnement pour Vercel

## Configuration pour le D√©ploiement

Lors du d√©ploiement sur Vercel, vous devez configurer ces variables dans :
**Settings ‚Üí Environment Variables**

---

## Option 1 : Hugging Face (GRATUIT - Recommand√© pour la d√©mo)

```env
LLM_PROVIDER=huggingface
HUGGINGFACE_API_KEY=hf_votre_cl√©_ici
```

### Obtenir votre cl√© Hugging Face :
1. Cr√©ez un compte sur https://huggingface.co (gratuit)
2. Allez sur https://huggingface.co/settings/tokens
3. Cliquez sur "New token"
4. Donnez-lui un nom (ex: "resume-matcher")
5. S√©lectionnez le type "Read"
6. Copiez le token (commence par `hf_`)

**Quota gratuit :** ~1000 requ√™tes/jour (largement suffisant pour une d√©mo)

---

## Option 2 : Mode Mock (Sans IA)

```env
LLM_PROVIDER=mock
```

**Avantages :**
- ‚úÖ Aucune cl√© API n√©cessaire
- ‚úÖ Fonctionne imm√©diatement
- ‚úÖ Bon pour tester l'interface

**Inconv√©nients :**
- ‚ùå Pas d'analyse IA r√©elle
- ‚ùå Scores bas√©s sur un algorithme simple

---

## Option 3 : OpenAI (Payant)

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-votre_cl√©_ici
```

**Co√ªt :** ~$0.002 par requ√™te (GPT-4)

---

## Option 4 : Groq (GRATUIT - Tr√®s rapide)

```env
LLM_PROVIDER=groq
GROQ_API_KEY=votre_cl√©_ici
```

### Obtenir votre cl√© Groq :
1. Cr√©ez un compte sur https://console.groq.com (gratuit)
2. Allez dans API Keys
3. Cr√©ez une nouvelle cl√©
4. Copiez la cl√©

**Avantage :** Tr√®s rapide (inference en quelques secondes)

---

## Variables Optionnelles

### Pour Hugging Face :
```env
HF_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

Mod√®les disponibles :
- `mistralai/Mistral-7B-Instruct-v0.2` (recommand√©)
- `meta-llama/Meta-Llama-3-8B-Instruct` (haute qualit√©)
- `google/gemma-2b-it` (le plus rapide)
- `HuggingFaceH4/zephyr-7b-beta` (excellent raisonnement)

### Pour Ollama (local uniquement) :
```env
LLM_PROVIDER=ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

‚ö†Ô∏è **Note :** Ollama ne fonctionne pas sur Vercel (n√©cessite un serveur local)

---

## üìã Checklist D√©ploiement

- [ ] Variable `LLM_PROVIDER` configur√©e
- [ ] Cl√© API ajout√©e (si n√©cessaire)
- [ ] Variables ajout√©es dans Vercel (pas dans le code !)
- [ ] Application test√©e apr√®s d√©ploiement

---

## üîí S√©curit√©

‚úÖ **√Ä FAIRE :**
- Ajouter les cl√©s dans Vercel (interface web)
- Garder les cl√©s secr√®tes
- Ne JAMAIS commiter `.env.local` dans Git

‚ùå **√Ä NE PAS FAIRE :**
- Hardcoder les cl√©s dans le code
- Partager les cl√©s publiquement
- Commiter les fichiers `.env*` dans Git

---

## üéØ Recommandation pour Votre Pr√©sentation

**Pour une d√©mo demain, utilisez Hugging Face :**

1. Inscription : 2 minutes
2. Obtention de la cl√© : 30 secondes
3. Configuration dans Vercel : 1 minute
4. **Total : ~4 minutes**

```env
LLM_PROVIDER=huggingface
HUGGINGFACE_API_KEY=hf_xxxxxxxxxxxxxxxxxxxx
```

‚úÖ Gratuit, rapide, et fonctionne parfaitement pour une d√©mo !

---

## üí° Tips

- Testez d'abord avec `LLM_PROVIDER=mock` pour v√©rifier que tout fonctionne
- Ajoutez ensuite la cl√© Hugging Face pour activer l'IA
- Gardez une copie de vos variables d'environnement dans un endroit s√ªr
